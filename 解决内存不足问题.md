# 解决 Ollama 内存不足问题

## 问题
模型 `gemma3:4b` 需要 4.9 GB 内存，但系统只有 2.9 GB 可用。

## 解决方案

### 方案 1: 释放系统内存（推荐）
1. 关闭其他占用内存的程序
2. 重启计算机释放内存
3. 然后再次尝试

### 方案 2: 使用更小的模型
下载并使用更小的模型，例如：

```bash
# 下载更小的模型（约 2-3 GB）
ollama pull llama3.2:1b    # 1B 参数模型
ollama pull phi3:mini      # 3.8B 参数模型（较小）
ollama pull qwen2:0.5b     # 0.5B 参数模型（最小）
```

然后在 `.env` 文件中设置：
```env
OLLAMA_MODEL=llama3.2:1b
# 或
OLLAMA_MODEL=phi3:mini
# 或
OLLAMA_MODEL=qwen2:0.5b
```

### 方案 3: 使用云端模式
如果本地内存不足，可以继续使用云端 DeepSeek API 模式：
- 在前端选择 "Cloud (High Performance)" 模式
- 不需要本地运行 Ollama

## 推荐操作
1. 先尝试释放内存（关闭其他程序）
2. 如果还是不够，下载更小的模型
3. 或者直接使用云端模式

